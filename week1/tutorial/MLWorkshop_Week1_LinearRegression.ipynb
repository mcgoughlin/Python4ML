{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcgoughlin/Python4ML/blob/main/week1/MLWorkshop_Week1_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_line(m,C):\n",
        "  x = np.linspace(-20,20)\n",
        "  def f(x, m=m, C=C):\n",
        "      return (x)*m + C\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.plot(x,f(x))\n",
        "  plt.ylim(-50,50)\n",
        "  plt.xlim(-20,20)"
      ],
      "metadata": {
        "id": "DIuhS_EJZJup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CazISR8X_HUG"
      },
      "source": [
        "# Coding Tutorial: Introduction to Python, Numpy, and Linear Regression\n",
        "\n",
        "Welcome to your first coding tutorial! \n",
        "\n",
        "In this tutorial, you will learn the basics of Python, the most popular programming language in the world, and how to manipulate data effectively within Python using the **NumPy** package. We will end the tutorial by learning to implement linear regression with one variable to predict the effect of COVID vaccines and lockdowns on COVID deaths in the South-East of England.\n",
        "\n",
        "Source of our COVID data: https://coronavirus.data.gov.uk/details/download\n",
        "\n",
        "\n",
        "# Outline\n",
        "- [ 1 - The Basics of Python](#1)\n",
        "  - [ 1.1 - Constants and Variables ](#1.1)\n",
        "  - [ 1.2 - Data Structures ](#1.2)\n",
        "  - [ 1.3 - Functions ](#1.3)\n",
        "  - [ 1.4 - Summary ](#1.4)\n",
        "- [ 2 - Linear regression with one variable ](#2)\n",
        "  - [ 2.1 Problem Statement](#2.1)\n",
        "  - [ 2.2 Dataset](#2.2)\n",
        "  - [ 2.3 Loss Function](#2.3)\n",
        "  - [ 2.4 Gradient Descent](#2.4)\n",
        "  - [ 2.5 COVID Exercise](#2.5)\n",
        "  - [ 2.6 Summary](#2.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"1\"></a>\n",
        "## 1 - Basics of Python\n",
        "<a name=\"1.1\"></a>\n",
        "### 1.1 Constants, Variables, and Operators\n",
        "\n",
        "Lets refresh our memory of basic mathematics, in the context of using Python.\n",
        "\n",
        "First, let's cover operators - adding, subtracting, multiplying and dividing. We will ask Python to 'print' - state the value -  of each operation below to determine the value of each of the following statements:\n"
      ],
      "metadata": {
        "id": "7nx-mWX6CLQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(5) ## print the value of 5\n",
        "print(5+2) ## print the value of 5 added to 2\n",
        "print(5*2) ## print the value of 5 multiplied by 2\n",
        "print(5*2 +3) ## print the value of 5 multiplied by 2 and then added to 3\n",
        "print(5*(2+5)) ## using brackets to order operations: print the value of 2 added to 5 and then multiplied by 5\n",
        "print(5**2) ## print the value of 5 to the power of 2\n",
        "print(5/2) ## print the value of 5 divided by 2"
      ],
      "metadata": {
        "id": "o2zSZ0JuYve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will see that the operations stated in the code block perform as expected. The order of operations obey BODMAS, PEMDAS, or whatever strange acronym you learned to order your mathematical operations in high school.\n",
        "\n",
        "The numbers used in this code are known as constants, as their value cannot change dynamically throughout the code. We cannot reassign the value of the number '5' - it is just 5. Lets perform operations on variables, whose values can and do change throughout code..."
      ],
      "metadata": {
        "id": "rYfaZ1RyZig-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "variable_one = 5\n",
        "print(\"Variable one's value is now\",variable_one) ## print the value of the internal statement - variable_one - which is previously defined as 5\n",
        "\n",
        "variable_two = variable_one*10 + 2\n",
        "print(\"Variable two's value is now\",variable_two) ## print the value of the new variable - variable_two\n",
        "\n",
        "variable_one = 0.1*(variable_one+variable_two) ## variable one's value has been changed here!\n",
        "print(\"Variable one's value is now\",variable_one)"
      ],
      "metadata": {
        "id": "0uQ8VWqGZ0Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We write variable names in a singular block of plain text - no spaces or punctuation besides '_' - and assign a value to it using the '=' sign, like so\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "variable_one = 5\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Using variables, we can manipulate the value of data passed through code dynamically, allowing generalisable software to be applied to data without having to know the data's literal values."
      ],
      "metadata": {
        "id": "ydxElOmGZzYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let us move on to a simple equation, predicated on these concepts of operators, contants and variables:"
      ],
      "metadata": {
        "id": "GgOS1qwhY3Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{align}\n",
        "Y &= mX + C \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "The equation above represents a simple linear formula that we may recognise from high school. An infinite number of possible linear curves can be produced by this formula. Test this out by running the code below and changing the values of $m$ and $C$. For now, only change the values of $m$ and $C$ to other small numbers, where -2 < m < 2 and -10 < C < 10. "
      ],
      "metadata": {
        "id": "jKEj6qtZYvzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m = -2\n",
        "C = 10\n",
        "\n",
        "#I wrote this 'plot_line' function before we started :)\n",
        "plot_line(m,C)"
      ],
      "metadata": {
        "id": "2L_vK9fyCQ4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traditionally, we think of X and Y as the variables in equation (1) ($X$ the independent variable, and $Y$ the dependent), and $m$ and $C$ as the constants that dictate the shape of the linear curve. However, our code above allows us to *vary* the values $m$ and $C$, making them *variables* in our code. \n",
        "\n",
        "### So what?\n",
        "\n",
        "Well, its important to recognise the variability of data in our code and software - variables can change value, and we must write software that can handle value changes in order to write good code. In reality, nearly all code in software is constituted by variables whose values can be unpredictable. Good software - good code - anticipates *how* input data and variables may change."
      ],
      "metadata": {
        "id": "adNxviFtZvK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code that fails\n",
        "To see how code can fail, change the values of $C$ in the below block to 100 or above. **Run the code block below.** \n",
        "\n",
        "Notice how our figure now fails to show a line? Both figures, produced by the same software,  have a fixed x-axis between $-20$ and $20$ and y-axis between $-50$ and $50$. Thus, when C exceeds 100, the plot cannot display our linear curve. Our software worked well for our neat initial example, where we bounded values of $m$ and $C$ to be small constants, but our software fails to generalise to all values of $C$ due to our poor anticipation of these values' variability."
      ],
      "metadata": {
        "id": "eV7qTAv5bDM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = 100\n",
        "\n",
        "plot_line(m,C)"
      ],
      "metadata": {
        "id": "XKQ61rQjbC-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"1.2\"></a>\n",
        "### 1.2 Data Structures\n",
        "\n",
        "Data structures are objects that hold data and enable efficient processing of  data. They are absolutely fundamental to writing good software. If you take one thing away from this first tutorial, let it be this:\n",
        "\n",
        "\n",
        "> **Good code is always predicated on the correct use of data structures.**\n",
        "\n",
        "\n",
        "Data structures allow coders to store and sort data in code, and perform efficient operations on all data within. In this section, you will see that conceptualising software writing as ***the manipulation of data structures*** will help you write better code, and satisfy the lesson of the first subsection:\n",
        "\n",
        "\n",
        "* 1.1 *Good code must be adaptive to variables to be useful in real-world settings.*\n",
        "\n",
        "\n",
        "First, let us consider the most basic data structure: the array. In the code block below, we import the **numpy** package, which is *the* python package for handling arrays. An array is simply an ordered set of values, where all values are stored with accessible using an index. For example, in the array $ [ 1, 4, -19, 32.2] $, the value $-19$ is accessed by selecting the array element at index 3, as $-19$ exists in the $3^{rd}$ position of the array.  The **numpy** package gives us the ability to create arrays, and all the functions we could wish to manipulate them with. \n",
        "\n",
        "Now, let's instantiate - create - an array full of numbers."
      ],
      "metadata": {
        "id": "_Z8mNou5nQAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "our_first_array = np.array([1.01, 102.54, -83.9, 0.0, 18.01, 22.304642,2.04])"
      ],
      "metadata": {
        "id": "tvnCGIWENW4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An array is simply an ordered list of numbers. Let's find out some information about this array:"
      ],
      "metadata": {
        "id": "Xpp54T81N-ui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Array max is\",np.max(our_first_array))\n",
        "print(\"Array min is\",np.min(our_first_array))\n",
        "print(\"Array median is\",np.median(our_first_array))\n",
        "print(\"Array mean is\",np.mean(our_first_array))\n",
        "print(\"Array standard deviation is\",np.std(our_first_array))"
      ],
      "metadata": {
        "id": "d2J-UdJLOZX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very cool - but what just happened?\n",
        "\n",
        "Well, all data structures are designed to do two things: **hold data** and **perform operations on data**. Our array is holding the numbers that we gave it, namely: $[1.01, 102.54, -83.9, 0.0, 18.01, 22.304642, 2.04]$. Our array, and the **numpy** package, comes with functions that help us manipulate this data.  These **numpy** functions are super fast and written by software geniuses - using packaged software and data structures is almost invariably the fastest and most efficiency way of processing data and writing software.\n",
        "\n",
        "So this is what we just did, in steps:\n",
        "\n",
        "1. We imported the **numpy** package, and renamed it 'np' in our code, for brevity.\n",
        "2. We instantiated an array, a data structure sourced from **numpy**, with the variable name 'our_first_array'. This array holds a list of numbers.\n",
        "3. We used **numpy** functions, which are accessed with '.' following 'np', to find out information about our_first_array.\n",
        "\n",
        "Okay fine, but what ***is*** a function?\n",
        "\n",
        "<a name=\"1.3\"></a>\n",
        "### 1.3 Functions\n",
        "\n",
        "Functions are any bit of code that receive inputs - enclosed within brackets following their name - and return outputs. For example:"
      ],
      "metadata": {
        "id": "GiWZ_IfSPLV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean([1,2,4,8,16,32])"
      ],
      "metadata": {
        "id": "d_m9ZifkWC4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The function above uses numpy's 'mean' function to calculate the mean of the following list of numbers: $[1,2,4,8,16,32]$, and returns its mean - the value 10.5. This is to say the function has ONE input - a list of numbers - and returns ONE output - a single number, corresponding to the list's mean. Many numpy functions can only be used on lists and arrays. Many numpy functions take and receive multiple inputs and outputs. Some can be performed on individual numbers, like the one below that calculates 2 to the power of the input"
      ],
      "metadata": {
        "id": "OLqw8ZoiWDMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp2(10)"
      ],
      "metadata": {
        "id": "5uOney4gVjTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To find numpy's full list of functions, run the code block below. I warn you though, there's a lot..."
      ],
      "metadata": {
        "id": "7pg5Doy0Vldf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(np))"
      ],
      "metadata": {
        "id": "PChI6XN_R_vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can write our own simple functions using the 'def' special word in Python! To construct a function of your own. simply write the following:\n",
        "\n",
        "```\n",
        "def Function_Name(Input1,Input2):\n",
        "  return Output1\n",
        "```\n",
        "\n",
        "Here, we have written a function, named Function_Name, that recieves inputs Input1 and Input2 and sends out Output1 using the special word 'return'.\n",
        "\n",
        "We always have to write our function's inputs in brackets following the function's name, and butt-end that line with a colon. In every subsequent line of code within the function, we must have an indent (use tab to create this on a keyboard) to indicate that line of code corresponds to the above function. The only way to make a function give an output is to use the special word 'return'. 'return' forces the function to output whatever follows in the same line of code, and then ends the function.\n",
        "\n",
        "Let's write our own simple adding function:"
      ],
      "metadata": {
        "id": "MCEPKH48KbgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adding_function(number1,number2):\n",
        "  ans = number1+number2\n",
        "  return \"The answer to {} + {} = {}\".format(number1,number2,ans)\n",
        "\n",
        "print(adding_function(1,2))\n",
        "print(adding_function(68924,2134))"
      ],
      "metadata": {
        "id": "BnxPid3qK0jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brilliant! Our adding function receives  two inputs, adds them together, and returns the answer in a text format that's amenable for printing. \n",
        "\n",
        "Now, lets try to write our linear equation (1) as a function, that recieves x as an input and returns y."
      ],
      "metadata": {
        "id": "rlH7vFl4LnKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_function(x):\n",
        "  m = 3\n",
        "  C = 5\n",
        "  return m*x + C\n",
        "\n",
        "print(\"X = 0, Y =\",linear_function(0))\n",
        "print(\"X = -15, Y =\",linear_function(-15))\n",
        "print(\"X = 524, Y =\",linear_function(524))"
      ],
      "metadata": {
        "id": "DxvHPH45LzN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "Try writing a function below that finds the 95% confidence range of an any-sized array. You can assume the array's values are distributed in a normal distribution. Do not use the np.percentile function!\n",
        "\n",
        "**HINT: The 97.5% and 2.5% values in a normal distribution are found using the following equations:**\n",
        "\n",
        "$$ 2.5\\% = mean - 1.96*\\frac{Std. Dev.}{\\sqrt{n}}$$\n",
        "$$ 97.5\\% = mean + 1.96*\\frac{Std. Dev.}{\\sqrt{n}}$$"
      ],
      "metadata": {
        "id": "A4gK2EGcre17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CI95(input_array):\n",
        "  #### WRITE YOUR CODE HERE\n",
        "\n",
        "  return confidence_interval"
      ],
      "metadata": {
        "id": "3bscjDp6OwKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once finished, test your function by running the function code block above, and then testing code block below:"
      ],
      "metadata": {
        "id": "Wmhx7gtVOyGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "array1 = np.random.randn(100)*np.random.randn(1) + np.random.randn(1)\n",
        "array2 = np.random.randn(1000)*np.random.randn(1) + np.random.randn(1)\n",
        "array3 = np.random.randn(10000)*np.random.randn(1) + np.random.randn(1)\n",
        "print(\"Test One Passed:\",CI95(array1)-((np.percentile(array1,97.5) - np.percentile(array1,2.5))/10) < 0.05,\"\\nPredicted CI: {:.3f}, Actual CI: {:.3f}\".format(CI95(array1),(np.percentile(array1,97.5) - np.percentile(array1,2.5))/10))\n",
        "print(\"\\nTest Two Passed:\",CI95(array2)-((np.percentile(array2,97.5) - np.percentile(array2,2.5))/31.62) < 0.01,\"\\nPredicted CI: {:.3f}, Actual CI: {:.3f}\".format(CI95(array2),(np.percentile(array2,97.5) - np.percentile(array2,2.5))/31.62))\n",
        "print(\"\\nTest Three Passed:\",CI95(array3)-((np.percentile(array3,97.5) - np.percentile(array3,2.5))/100) < 0.01,\"\\nPredicted CI: {:.3f}, Actual CI: {:.3f}\".format(CI95(array3),(np.percentile(array3,97.5) - np.percentile(array3,2.5))/100))\n"
      ],
      "metadata": {
        "id": "NzG94XPvsUQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"1.4\"></a>\n",
        "### 1.4 Summary\n",
        "\n",
        "In this opening section, we have covered a lot. We have covered:\n",
        "1. Operators\n",
        "2. Contants and Variables\n",
        "3. Data Types\n",
        "4. Data Structures\n",
        "5. Functions\n",
        "\n",
        "This is everything that you need for programming the most basic supervised machine learning algorithm - linear regression! Congratulations, you can now start performing some basic machine learning. \n",
        "\n",
        "To perform more advanced machine learning techniques, you will need to learn more advanced programming concepts. This seem likes a fair deal to me. In the first half of every tutorial, you will be introduced to programming concepts that will help you in the second half of every tutorial, where you will learn increasingly sophisticated machine learning techniques. There are no prerequisites for this course, except having completed high-school mathematics. This course will end with you learning how to train a neural network. I wish you the best of luck on your learning adventure!"
      ],
      "metadata": {
        "id": "NVCN-qoLeb7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2\"></a>\n",
        "## 2 - Learning to Perform Linear Regression with One Variable\n",
        "\n",
        "<a name=\"2.1\"></a>\n",
        "## 2.1 - Problem Statement\n",
        "Linear regression models fit straight lines to data, in an attempt to model the relationships between two variables with a linear equation. To understand this, let us revisit equation (1) from section 1.1:\n",
        "\n",
        "\\begin{align}\n",
        "Y &= mX + C \\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "Let's define this more formally: given some values of X and Y, our regression algorithm finds the values of $m$ and $C$ that produce the 'most correct' line that emulates the relationship between X and Y. The best values of $m$ and $C$ are those that best capture the linear relationship between X and Y. **Run the code block below to see how this might look in practise** ."
      ],
      "metadata": {
        "id": "AMKChg91lEu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt #a package that helps us plot data\n",
        "\n",
        "m  = 3 ## I am cheating here!\n",
        "C = 5 ## I am cheating here!\n",
        "X = np.arange(-20,20)\n",
        "Y = m*X + C + np.random.uniform(low=-30,high=30,size=(40))\n",
        "\n",
        "regression = m*X + C\n",
        "\n",
        "plt.scatter(X,Y)\n",
        "plt.plot(X,regression,c='r',label='Linear Relationship between X and Y')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3nQJrT9OlyBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, in the above example we are cheating! We know the correct values of $m$ and $C$ before starting - we even stated them in the 4th and 5th line of code. Linear regression only provides utility when we do **not** know the correct values of $m$ and $C$, and we wish to find them.\n",
        "\n",
        "Linear regression is useful when we have some observed data, and we want to abstract general relationships between the data's variables. These abstractions allow us to the predict values of our data at points where we have no observations. To do this, we will need to understand a few new machine learning concepts."
      ],
      "metadata": {
        "id": "R-4ghiebnSuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2.2\"></a>\n",
        "## 2.2 - Datasets\n",
        "\n",
        "A dataset is a set variables observed in different states. Traditionally, datasets are displayed as a table, where columns refer to variables and rows refer to observations.\n",
        "\n",
        "In this tutorial we will look at COVID data relating to the South East of England. Specfically, we will be looking at how vaccination rates, case rates, and death rates change over time. First, we must load the data from a file. Google Colab allows us to store files in the same environment as this notebook. We are going to store our data as a '.csv' file named 'covid_data.csv', store it in our 'sample_data' folder, and load it into our python script using the **pandas** package. **Pandas** is brilliant for manipulating and loading datasets in python.\n",
        "\n",
        "You should have been provided with a file called 'covid_data.csv' ahead of this tutorial - either in an email, or on this course's GitHub page. Please upload this file to the 'sample_data' folder in this colab notebook's environment by going into the 'files' section of the left-hand toolbar, and uploading using the 'upload to session storage' button. Once uploaded, ensure that 'covid_data.csv' is in the 'sample_data' folder.\n",
        "\n",
        "**Run the code below to ensure that the file is in the correct location**."
      ],
      "metadata": {
        "id": "B8dcIwrIosdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "covid_dataset = pd.read_csv(\"sample_data/covid_data.csv\")\n",
        "print(\"File correctly read!\")"
      ],
      "metadata": {
        "id": "snvhzF0DqS2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can inspect the first five rows of our dataset using the 'head' function:"
      ],
      "metadata": {
        "id": "rElKwi0ztsk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(covid_dataset.head())\n",
        "print(\"\\nCovid Dataset has {} rows.\".format(covid_dataset.shape[0]))"
      ],
      "metadata": {
        "id": "0WWPxYkJt9m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because our dataset is real, it is messy. In some observations, we are missing values for some variable. In these places, we have records with no entries marked 'NaN' - not a number. Lets do a little spring cleaning and remove all rows that contain a NaN value..."
      ],
      "metadata": {
        "id": "qXL7qTG5uCOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "covid_dataset_clean = covid_dataset.dropna() # drop rows with missing values\n",
        "covid_dataset_clean['Date'] = pd.to_datetime(covid_dataset_clean['Date'],dayfirst=True) # convert the date format into a python-readable time format\n",
        "covid_dataset_clean['DaysSinceRecordsBegan'] = (covid_dataset_clean['Date'] - covid_dataset_clean['Date'].iloc[-1]).dt.days # create new variable to organise data - days since records began\n",
        "\n",
        "print(covid_dataset_clean.head())\n",
        "print(\"\\nCovid Dataset has {} rows.\".format(covid_dataset_clean.shape[0]))"
      ],
      "metadata": {
        "id": "q4PhoVdMuP7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hm... our cleaning dropped $976 - 693 = 283$ rows. This might not be ideal, but we will roll with it for now.\n",
        "\n",
        "###Preprocessing\n",
        "\n",
        "Data cleaning - preprocessing - is an enormously important aspect of machine learning. Here, we are simply discarding all data that isn't perfectly easy to manipulate - this is poor practise. In the future, we will learn better ways of making up the knowledge gaps extant within 'NaN' entries. However, for now, our primitive preprocessing approach has given us a clean dataset. Lets interrogate!\n",
        "\n",
        "### Visualising\n",
        "\n",
        "Before beginning a machine learning task, ***always visualise the data!!!*** I cannot emphasise the importance of data visualisation enough. Visualising data helps us build intuition that guides our machine learning model development, and helps us validate the model's performance after development. \n",
        "\n",
        "Here, we visualise the data using matplotlib - Python's premier visualisation package. These functions used in the immediate code block are complex - DO NOT WORRY - we will build from more simple matplotlib functions after this initial visualisation, to build intuition:"
      ],
      "metadata": {
        "id": "dbd8PqTauyZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(12,8))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "lns1 = ax.plot(covid_dataset_clean['DaysSinceRecordsBegan'], covid_dataset_clean['CumCases'], 'r-.', label = 'Cumulative Cases')\n",
        "lns2 = ax.plot(covid_dataset_clean['DaysSinceRecordsBegan'], covid_dataset_clean['CumVaccinations'], 'b--', label = 'Cumulative Vaccinations')\n",
        "ax2 = ax.twinx()\n",
        "lns3 = ax2.plot(covid_dataset_clean['DaysSinceRecordsBegan'], covid_dataset_clean['CumDeaths'], 'k', label = 'Cumulative Deaths')\n",
        "\n",
        "# added these three lines\n",
        "lns = lns1+lns2+lns3\n",
        "labs = [l.get_label() for l in lns]\n",
        "ax.legend(lns, labs, loc=0,fontsize='x-large')\n",
        "\n",
        "ax.grid()\n",
        "ax.set_xlabel(\"Time (Days)\",fontsize='x-large')\n",
        "ax.set_ylabel(r\"Number of Cumulative Cases and Vaccinations\",fontsize='x-large')\n",
        "ax2.set_ylabel(r\"Number of Cumulative Deaths\",fontsize='x-large')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AdPub88u0JAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WOW! Cool right?\n",
        "\n",
        "Let's plot something simpler. Here, we call upon our dataset variable name - entitled 'covid_dataset_clean' - and plot the column 'CumCases' on the x-axis and 'CumDeaths' on the y-axis:"
      ],
      "metadata": {
        "id": "hMlA9Jbq-rrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(covid_dataset_clean['CumCases'],covid_dataset_clean['CumDeaths'])\n",
        "plt.xlabel('Cases')\n",
        "plt.ylabel('Deaths')"
      ],
      "metadata": {
        "id": "s31cVKRI-8Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fascinating! The relationship between cumulative deaths and cumulative cases goes through something of a phase shift at roughly $0.52e^6$ cases.\n",
        "\n",
        "Might we hazard a guess and assume:\n",
        "\n",
        "> **The phase shift observed in our second graph corresponds to successful COVID mitigation policy (lockdowns preventing cumulative COVID case growth) and increased vaccination rates**.\n",
        "\n",
        "If we assume this, perhaps we can use this apparent phase shift to estimate the number of lives saved by these population-level interventions, by plotting a linear regression model in each phase, and comparing the rate of deaths:cases before and after this phase shift.\n",
        "\n",
        "To do this, lets learn how to create a Linear Regression Model. First things first, let's learn about the cost function...\n",
        "\n",
        "**DISCLAIMER**: Obviously, there are other very real variables that are not included in this analysis, such as the ever-evolving nature of COVID variants, the epidemiology of the disease, and the general increases in sanitary awareness by the population. This analysis is PURELY meant as an informative learning exercise, that gives some insight into how machine learnings models are used in practise, and ***definitely not***  an appraisal of UK government policy during COVID."
      ],
      "metadata": {
        "id": "OKC2Ex1rACee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2.3\"></a>\n",
        "## 2.3 - Cost Function\n",
        "\n",
        "The cost function is a mathematical function that evaluates the difference between two numbers, or two sets of numbers. In supervised machine learning, we use cost functions to evaluate how wrong a prediction is versus a correct answer, known as a label. Given a label and a prediction, both in the form of numbers, the cost function evaluates how incorrect the prediction is by comparing the difference between the two. Higher cost = more incorrect, Lower cost = less incorrect.\n",
        "\n",
        "Lets look at our first cost function, Squared Error Loss (SE Loss):\n",
        "\n"
      ],
      "metadata": {
        "id": "d1dSiz7fDA4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\\begin{align}\n",
        "SE &= \\frac12(label - pred)^{2} \\tag{2}\n",
        "\\end{align}\n",
        "\n",
        "The equation above represents a simple quadratic formula. Without trying to tease the algebra of this equation apart, let us visualise SE Loss over a range of possible predictions, looking at the case where the label has a value of 1. ***Run the code block below***"
      ],
      "metadata": {
        "id": "PSa-sE01E5ON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pred = np.arange(-10,10)\n",
        "label = np.ones(20)\n",
        "\n",
        "SE = 0.5*np.square(label-pred)\n",
        "\n",
        "plt.plot(pred,SE,label='SE Loss')\n",
        "plt.xlim(-5,7)\n",
        "plt.ylim(-1,20)\n",
        "plt.xlabel('Prediction')\n",
        "plt.ylabel('SE Loss')\n",
        "plt.vlines(1,-1,20,label='Correct Answer', color='r',linestyle='--')\n",
        "plt.legend(loc = 1)\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "msCG5qotGJvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that over the range of possible predictions on the x-axis, the prediction that produces the smallest SE Loss is where prediction = 1, which is our label. SE loss = 0 when the prediction =  label. Furthermore, we can see that a guess that's too small (prediction = -2) is equally as bad as a prediction thats too large by the same amount (prediction = 4), as both of these predictions incur the same SE loss value.\n",
        "\n",
        "Cool! We have a method of expressing incorrectness... how do we use this?"
      ],
      "metadata": {
        "id": "LCuLDDIvJfEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2.4\"></a>\n",
        "## 2.4 - Gradient Descent\n",
        "\n",
        "Now, you will have to bear with me. I am going to ask you to visualise one instance of equation (1) as a node. See this instance below, with values $m = 0.65$ and $C = -2$. \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "K-KFrE5CRLC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This node is our linear regression model. \n",
        "\n",
        " $m$ and $C$ are properties of this model. Equation (1) defines every single possible straight line that can exist in two dimensions, whereas this node - our model - defines a singular linear equation, defined by the node's $m$ and $C$ values.\n",
        "\n",
        "We could rewrite this node as\n",
        "\n",
        "$$ Y = 0.65X - 2 $$\n",
        "\n",
        "But we will not, because the node's values of $m$ and $C$ can change - they are variable. It is for this reason that we conceptualise the model as a node, and its variables $m$ and $C$ as the node's properties. To perform linear regression, we tune our model's values of $m$ and $C$ using observed data, such that our model produces the lowest possible loss over the totality of our observed data.\n",
        "\n",
        "This tuning process is known as gradient descent, and I am going to ask you to run this process in code. \n",
        "\n",
        "I will write the cost function and gradient descent functions for you, give you the observed data and give you a node with two values as a model. I want you to optimise the model by entering the correct inputs into the marked code block and running all the code below. Do this by reading the code for the 'gradient_descent' function, and entering the correct inputs in the following code block where prompted."
      ],
      "metadata": {
        "id": "R0CawaXMVAIT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate our data **run this code block:**"
      ],
      "metadata": {
        "id": "bbAdhjvyYtT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.random.randn(1000)\n",
        "Y = X*np.random.randn(1)*3 + np.random.randn(1000)*np.random.randn(1000)*np.random.randn(1) + np.random.randn(1)\n",
        "plt.scatter(X,Y)"
      ],
      "metadata": {
        "id": "4WLjoEkvYAtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create our loss and gradient descent functions, **run the following code block**. \n",
        "\n",
        "***Read the first line of the gradient_descent function code to understand the necessary inputs for this function***.\n",
        "\n",
        "Some definitions to help you understand this code:\n",
        "\n",
        "**Batch size**: In machine learning, batch size refers to the number of observations that we train on simultaneously. We compute the loss of each observation in the batch at the same time and average the results, to reduce the effects of nosie in a single observation and increase training speed.\n",
        "\n",
        "**Epoch**: In machine learning, an epoch refers to one run-through of learning through a whole training dataset. If you have 100 observations, and a batch size of 5, one epoch refers to 20 batchs of the training data, such that all observations have been seen once. In many cases, we have to run through the training dataset multiple times, for multiple epochs, to ensure training is completed.\n",
        "\n",
        "**Learning rate (LR)**: The size of gradient / weight adjustment following each training batch. A higher learning rate increases the speed of training, but also decreases the precision of optimisation, making training less likely to fully converge."
      ],
      "metadata": {
        "id": "3taak-SFoOsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(label,pred):\n",
        "\n",
        "  return 0.5 * (((label-pred)**2)**0.5)\n",
        "\n",
        "def gradient_descent(X,Y,model,loss_function):\n",
        "  epochs = 10000\n",
        "  batch_size = 100\n",
        "  data_size = len(X)\n",
        "  lr = 0.01\n",
        "\n",
        "  losses = []\n",
        "  model_mvals =[]\n",
        "  model_cvals =[]\n",
        "\n",
        "  for i in range(epochs): # repeat training process for every epoch\n",
        "\n",
        "      indices = np.random.randint(0,data_size, batch_size) # randomly generate indices for creating batches of data\n",
        "      X_batch, labels = X[indices],Y[indices] # create data batches for training\n",
        "      preds = model[0]* X_batch + model[1] # generate model predictions for input data\n",
        "      # learning_rate = (lr - lr*((epochs-i)/epochs)+1e-4)\n",
        "      loss = loss_function(labels,preds)\n",
        "      loss = np.mean(loss)/np.mean(np.abs(preds))\n",
        "      # print(\"Epoch {}/{}\".format(i,epochs), end=\"\\r\")\n",
        "      if loss>1e20: raise Exception(\"Loss is too high\")\n",
        "\n",
        "\n",
        "      ### This is the maths of the gradient descent algorithm, and is out of the scope of this course\n",
        "      grad_C = np.mean(preds-labels)\n",
        "      grad_m = np.mean(X_batch * (preds - labels).T)\n",
        "\n",
        "      new_m = model[0] - grad_m*loss*lr\n",
        "      new_C = model[1] - grad_C*loss*lr\n",
        "\n",
        "      model[0] = new_m\n",
        "      model[1] = new_C\n",
        "\n",
        "      losses.append(loss)\n",
        "      model_mvals.append(model[0])\n",
        "      model_cvals.append(model[1])\n",
        "\n",
        "  print(\"Gradient Descent Complete\")\n",
        "  return losses, model_mvals, model_cvals,model\n"
      ],
      "metadata": {
        "id": "vCTorgtcZ-87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now start gradient descent! Enter the correct inputs into the gradient descent function to initiate machine learning.\n",
        "\n",
        "To initiate machine learning **enter the correct inputs to the gradient descent function and then run this code block**:"
      ],
      "metadata": {
        "id": "U4fOuBBXoTpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#we initiate our model with some small random values\n",
        "model = np.random.randn(2)/10\n",
        "\n",
        "#remember, our data is stored in the variables X and Y\n",
        "# losses,ms,cs,model = gradient_descent( #### INSERT INPUTS TO THE GRADIENT DESCENT ALGORITHM HERE ### ) \n",
        "losses,ms,cs,model = gradient_descent(X,Y... # REMOVE ELLIPSES AND WRITE CODE HERE!\n",
        "                                      \n",
        "                                      \n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Training Iterations\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "O3GEsP2tf0GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "preds = model[0]*X + model[1]\n",
        "\n",
        "plt.scatter(X,Y)\n",
        "plt.plot(X,preds,c='r')"
      ],
      "metadata": {
        "id": "6uDONwx8j4NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have done this stage correctly, your red line will closely follow the blue dots that represent our observed data. If this is what you see - **well done!**\n",
        "\n",
        "Let's clear up interesting things that happened in our code that may not have been immediately apparent..\n",
        "\n",
        "### Mean-Squared Error\n",
        "\n",
        "You will see that, in our gradient_descent function, we take the mean of our loss function over a range of samples. Infact, we never use individual loss values anywhere in the code. If we iteratively trained using one single data point at a time, our training process would be very slow and perhaps prohibitively noisy. We'd be training sometimes on data far above and below the correct distribution, and perhaps never settle on the correct values of $m$ and $C$. \n",
        "\n",
        "To avoid singular data point training, we take batches of data points and calculate the mean loss of our model across all these points. Hence, in reality, we use a slightly different loss equation to that seen in equation (2) - we use the *mean* squared error (MSE) loss:\n",
        "\n",
        "\\begin{align}\n",
        "MSE &= \\sum_{i=1}^{n} \\frac{\\frac12(label_i - pred_i)^{2}}{n} \\tag{3}\n",
        "\\end{align}\n",
        "\n",
        "Don't worry that this equation looks rather intimidating. It means: \n",
        "\n",
        ">*The sum of losses over a batch of $n$ data points, divided by n*\n",
        "\n",
        "Which is identical to saying the average of SE losses over a batch (sample) of data. \n",
        "\n",
        "### Describing the Gradient Descent Algorithm.\n",
        "\n",
        "This course does not cover the calculus of the gradient descent algorithm; instead, we describe the algorithmic process in words. For those wanting to understand the calculus of gradient descent in more detail, watch these short videos (40 minutes in total):\n",
        "[\n",
        "Gradient Descent ](https://www.youtube.com/watch?v=IHZwWFHWa-w), [Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U), and [Backpropagation Calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8). **This knowledge is absolutely not necessary for this course.** \n",
        "\n",
        "### Understanding Gradient Descent\n",
        "We describe the gradient descent algorithm below. This algorithm tunes a model's parameters, to produce a model that makes predictions on data with low loss.\n",
        "\n",
        "1. **Loss and Label Prediction** When making a prediction on a sample (or a batch), we calculate the model's incorrectness by calculating the MSE (eqn. 3) of all the model's answers in comparison to the sample's labels. \n",
        "\n",
        "2. **Model Parameters** We know that our model has parameters ($m$ and $C$). If we look a eqn (1), we can see that our predicted $Y$ is a function of our input $X$, and our model parameters $m$ and $C$. This means, according to eqn. (2), that our model's loss is a function of: a) input $X$, b) our label, and c) our model parameters (in this case, $m$ and $C$).\n",
        "\n",
        "3. **Loss is caused by faulty Model Parameters** If we make the assumption that $X$ and $Y$ are constants - perfectly measurable quantities observed with no uncertainity - then our loss is no longer a function of these values. This isolates the portion of loss - incorrectness - associated with our model to the model's parameters. Practically, this is done in training by taking each batch of input samples, keeping $X$ and $Y$ constant, and making small alterations in the model's parameters ($m$ and $C$ values), and seeing how loss varies with variation's in the model's parameters. \n",
        "\n",
        "4. **Gradient Descent** By looking at how the model's loss changes with respect to the model's parameters, we can choose to adjust the model's parameters in the direction that incurs the lowest loss. This process is termed 'gradient descent', because we are finding the gradient of the ***loss*** with respect to the ***model's parameters*** and heading ***down the gradient - choosing model parameters that minimise the loss***.\n",
        "\n",
        "\n",
        "The fourth point is visualised below - each dot corresponds to a batch of inputs that our previous linear regression model reviewed, with its corresponding model parameter values and losses. Loss is reduced over time by following the gradient of the loss function (With respect to model parameters $m$ and $C$) down the curve until its minimum - the line represents the sequential path of $m$ and $C$ values the the model took in its attempts to minimise loss.\n",
        "\n"
      ],
      "metadata": {
        "id": "ha5npKlhzZvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot\n",
        "\n",
        "fig = plt.figure(figsize=(24,8))\n",
        "ax1 = fig.add_subplot(131,projection='3d')\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax3 = fig.add_subplot(133)\n",
        "\n",
        "ax1.scatter(ms,cs,losses, c = losses,s=5)\n",
        "ax1.set_xlabel('Model parameter \\'m\\'')\n",
        "ax1.set_ylabel('Model parameter \\'C\\'')\n",
        "ax1.set_zlabel('Loss')\n",
        "ax1.view_init(elev=45,azim=30)\n",
        "ax1.plot(ms,cs,losses)\n",
        "\n",
        "ax2.scatter(cs,losses, c = losses,s=15)\n",
        "ax2.set_xlabel('Model parameter \\'C\\'')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.plot(cs,losses,linewidth=0.6)\n",
        "\n",
        "ax3.scatter(ms,losses, c = losses,s=15)\n",
        "ax3.set_xlabel('Model parameter \\'m\\'')\n",
        "ax3.set_ylabel('Loss')\n",
        "ax3.plot(ms,losses,linewidth=0.6)"
      ],
      "metadata": {
        "id": "21QjDuxHaCbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch Size and Learning Rate\n",
        "\n",
        "When we make adjustments to the model's parameters, we use three values to guide us: the loss, the gradient, and the *learning rate*.\n",
        "\n",
        "Adjustments to our model parameters must be a function of loss to allow our model to converge on the correct parameters. If loss is low, we want our model parameter adjustments to be small, and not fly off the handle away from our nearly-converged state.\n",
        "\n",
        "Adjustments to our model parameters must also be a function of the gradient between loss and the parameters, to tune our model's parameters in the direction of *reducing loss*.\n",
        "\n",
        "Finally, adjustments to our model parameters are a function of the model's learning rate. This is a user-defined value ***(we choose the learning rate!)***, that dictates the rate of learning. A higher learning rate causes quicker training, as the model makes larger adjustments to the model parameter's with every training batch, but it also makes training more unstable. As mentioned above, if our model adjustments are too large, our model may not converge during training.\n",
        "\n",
        "Putting these ideas into an equation, after training batch $i$, our model parameters can be described like so:\n",
        "\n",
        "\\begin{align}\n",
        "Parameters_{i+1} &= Parameters_i - LearningRate * Loss_i * \\frac{\\Delta Parameters_i}{\\Delta Loss_i} \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "where the second term's $\\frac{\\Delta Parameters_i}{\\Delta Loss_i}$ portion is the gradient between the model's parameters and the model's loss after training batch $i$. This is codified in the gradient_descent function written above, in line 23-23\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "      new_m = model[0] + grad_m*learning_rate*np.mean(loss)\n",
        "      new_C = model[1] + grad_C*learning_rate*np.mean(loss)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "wCfY3sMChRrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2.5\"></a>\n",
        "## 2.5 - COVID Example\n",
        "\n",
        "Now, let's finally set you up with our COVID exercise! Like we said, we will chop up the COVID data into to sections at cases = $0.52e^6$. I want you to perform a linear regression on both datasets.\n",
        "\n",
        "Our data:"
      ],
      "metadata": {
        "id": "9Y99BZVAspSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(covid_dataset_clean['CumCases'],covid_dataset_clean['CumDeaths'])\n",
        "plt.xlabel('Cases')\n",
        "plt.ylabel('Deaths')"
      ],
      "metadata": {
        "id": "qlXT5eOHs_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First half, where number of cases is less than $0.52e^6$:\n",
        "\n",
        "(We apply some preprocessing here, dividing our X data by 35000 and y data by 350. Don't worry about this, this is a preprocessing step that we will explain in tutorial 3)."
      ],
      "metadata": {
        "id": "-McdLA_vtQ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = covid_dataset_clean['CumCases'][covid_dataset_clean['CumCases']<0.52e6].values/35000 ## We split the data at less than 0.52e6 and apply a normalising division\n",
        "Y1 = covid_dataset_clean['CumDeaths'][covid_dataset_clean['CumCases']<0.52e6].values/350 ## We split the data at less than 0.52e6 and apply a normalising division\n",
        "plt.scatter(X1,Y1)\n",
        "plt.xlabel('Normalised Cases')\n",
        "plt.ylabel('Normalised Deaths')"
      ],
      "metadata": {
        "id": "ZP0-LEXqtQo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second half, where number of cases exceed $0.52e^6$.\n",
        "\n",
        "(Again, we apply some preprocessing parameters here - this will be explained later in the course.)"
      ],
      "metadata": {
        "id": "LSrlB07EtsG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X2 = covid_dataset_clean['CumCases'][covid_dataset_clean['CumCases']>0.52e6].values/2e5 ## We split the data at greater than 0.52e6 and apply a normalising division\n",
        "Y2 = covid_dataset_clean['CumDeaths'][covid_dataset_clean['CumCases']>0.52e6].values/200 ## We split the data at greater than 0.52e6 and apply a normalising division\n",
        "plt.scatter(X2,Y2)\n",
        "plt.xlabel('Normalised Cases')\n",
        "plt.ylabel('Normalised Deaths')"
      ],
      "metadata": {
        "id": "35kfyJ4NtyUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Exercise\n",
        "\n",
        "In the dedicated space, please enter your solution to the following problem: **find the model parameter values for a linear regression on each split of the data**. The model for cases less than $0.52e^6$ should have the variable name $model1$, and the corresponding data is called $X1$ and $Y1$. The model for cases exceeding $0.52e^6$ should have the variable name $model2$, and its corresponding data is called $X2$ and $Y2$.\n",
        "\n"
      ],
      "metadata": {
        "id": "Gbtb6mwiuJcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Starting you off with a clue! We initiate model1 and model2 for you, and set up some of the\n",
        "### code for starting gradient descent.\n",
        "\n",
        "###\n",
        "import numpy as np\n",
        "model1 = np.random.randn(2)\n",
        "model2 = np.random.randn(2)\n",
        "\n",
        "losses1,ms1,cs1,model1 =gradient_descent(X1,Y1 ###Enter the rest of the inputs!\n",
        "losses2, ...                                   ##Repeat for model2!"
      ],
      "metadata": {
        "id": "GLe003MZuIaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model1,model2)"
      ],
      "metadata": {
        "id": "ksMEqa7Dyafp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Test\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.plot(covid_dataset_clean['CumCases'],covid_dataset_clean['CumDeaths'],linewidth=3, label='Real COVID Data')\n",
        "\n",
        "x_norm1 = covid_dataset_clean['CumCases'].values/35000\n",
        "x_norm2 = covid_dataset_clean['CumCases'].values/2e5\n",
        "preds1 = model1[0]*x_norm1 + model1[1]\n",
        "preds2 = model2[0]*x_norm2 + model2[1]\n",
        "preds1*=350\n",
        "preds2*=200\n",
        "plt.plot(covid_dataset_clean['CumCases'],preds1,linestyle='--',color='r',linewidth=2,label = \"Pre-intervention death prediction\")\n",
        "plt.plot(covid_dataset_clean['CumCases'],preds2,linestyle='--',color='g',linewidth=2,label = \"Post-intervention death prediction\")\n",
        "plt.vlines([covid_dataset_clean['CumCases'].max(),covid_dataset_clean['CumCases'].max()],\n",
        "           [0,preds2[0]],[preds2[0],preds1[1]],linestyles=['-','dotted'],linewidths=3.5,label = \"Total Deaths (Solid)\\nLives Saved (Dotted)\")\n",
        "plt.xlim(0,covid_dataset_clean['CumCases'].max()*1.05)\n",
        "plt.ylim(0,max(covid_dataset_clean['CumDeaths'].max(),preds2.max(),preds1.max()*1.05))\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('Number of Cases (millions)')\n",
        "plt.ylabel('Number of Deaths')\n",
        "\n",
        "lives_saved_thousands = int((preds1[0]-preds2[0])/1000)\n",
        "\n",
        "bool1 = lives_saved_thousands > 73\n",
        "bool2 = lives_saved_thousands < 77\n",
        "print(\"Test passed: {}\".format(bool1 and bool2))\n",
        "print(\"Based on our linear regression model, it is estimated that {},000 lives were saved by COVID policies in the South-East of England.\\n\".format(lives_saved_thousands))"
      ],
      "metadata": {
        "id": "OiD1eSAowo3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"2.4\"></a>\n",
        "## 2.6 - Summary\n",
        "\n",
        "If you completed the above exercise - congratulations! You have trained your first machine learning models. You learned:\n",
        "\n",
        "1. How we measure the 'correctness' of our prediction during training, using the loss function.\n",
        "2. How the gradient descent algorithm optimises machine learning models.\n",
        "3. How batch size and learning rate effects training.\n",
        "4. The importance of data visualisation and preprocessing before diving into machine learning.\n",
        "\n",
        "These lessons are enormously important in any algorithm that trains in gradient descent, including the state-of-the-art AI in computer vision and natural language processing, such as Transformers and Convolutional Neural Networks.\n",
        "\n",
        "In the next tutorial, we will learn to to perform regression and classification on multiple variables. This will allow us to make highly sophisticated predictions that take into account multiple variables, and their interactions, simultaneously."
      ],
      "metadata": {
        "id": "a_2wy2QHgh_p"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}